{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Rocket Transformer Classifier\n",
    "class RocketTransformerClassifier:\n",
    "    def __init__(self):\n",
    "        self.classifiers_mapping = {}\n",
    "\n",
    "    def fit_rocket(self, x_train, y_train, kernels=10000):\n",
    "        # Initialize and fit Rocket transformer\n",
    "        rocket = Rocket(num_kernels=kernels, normalise=False)\n",
    "        rocket.fit(x_train)\n",
    "        x_training_transform = rocket.transform(x_train)\n",
    "\n",
    "        # Normalize the transformed data\n",
    "        scaler = StandardScaler()\n",
    "        x_training_transform = scaler.fit_transform(x_training_transform)\n",
    "\n",
    "        # Train RidgeClassifier with normalized transformed data\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        classifier.fit(x_training_transform, y_train)\n",
    "\n",
    "        # Store the transformer, scaler, and classifier\n",
    "        self.classifiers_mapping[\"transformer\"] = rocket\n",
    "        self.classifiers_mapping[\"scaler\"] = scaler\n",
    "        self.classifiers_mapping[\"classifier\"] = classifier\n",
    "\n",
    "    def evaluate(self, x_val, y_val):\n",
    "        rocket = self.classifiers_mapping[\"transformer\"]\n",
    "        scaler = self.classifiers_mapping[\"scaler\"]\n",
    "        classifier = self.classifiers_mapping[\"classifier\"]\n",
    "    \n",
    "        # Transform and normalize test data\n",
    "        x_val_transform = rocket.transform(x_val)\n",
    "        x_val_transform = scaler.transform(x_val_transform)\n",
    "    \n",
    "        # Predict and evaluate\n",
    "        predictions = classifier.predict(x_val_transform)\n",
    "        accuracy = metrics.accuracy_score(y_val, predictions)\n",
    "\n",
    "        logger.info(\"-----------------------------------------------\")\n",
    "        logger.info(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def predict_rocket(self, x_test, y_test):\n",
    "        # Retrieve transformer, scaler, and classifier\n",
    "        rocket = self.classifiers_mapping[\"transformer\"]\n",
    "        scaler = self.classifiers_mapping[\"scaler\"]\n",
    "        classifier = self.classifiers_mapping[\"classifier\"]\n",
    "    \n",
    "        # Transform and normalize test data\n",
    "        x_test_transform = rocket.transform(x_test)\n",
    "        x_test_transform = scaler.transform(x_test_transform)\n",
    "    \n",
    "        # Predict and evaluate\n",
    "        predictions = classifier.predict(x_test_transform)\n",
    "        accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "        confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "        classification_report = metrics.classification_report(y_test, predictions)\n",
    "    \n",
    "        logger.info(\"-----------------------------------------------\")\n",
    "        logger.info(f\"Accuracy: {accuracy}\")\n",
    "        logger.info(\"\\nConfusion Matrix:\\n\" + str(confusion_matrix))\n",
    "        logger.info(\"\\nClassification Report:\\n\" + classification_report)\n",
    "    \n",
    "        return accuracy, confusion_matrix, classification_report\n",
    "    \n",
    "    \n",
    "def pad_with_last_row(series, fixed_rows):\n",
    "    \"\"\"\n",
    "    Pad time-series data with the last row to ensure a fixed number of rows.\n",
    "    Truncate rows if too many, pad with the last row if too few.\n",
    "    \"\"\"\n",
    "    series = np.array(series)\n",
    "    current_rows, columns = series.shape\n",
    "\n",
    "    # Truncate if too many rows\n",
    "    if current_rows > fixed_rows:\n",
    "        return series[:fixed_rows, :]\n",
    "\n",
    "    # Pad with the last row if too few rows\n",
    "    if current_rows < fixed_rows:\n",
    "        last_row = series[-1, :]  # Get the last row\n",
    "        padding = np.tile(last_row, (fixed_rows - current_rows, 1))  # Repeat the last row\n",
    "        return np.vstack((series, padding))  # Add padding rows\n",
    "\n",
    "    return series\n",
    "\n",
    "# Define function to load time-series data\n",
    "def load_time_series_data(input_dir):\n",
    "    \"\"\"\n",
    "    Load all time-series CSV files in the input directory and prepare\n",
    "    the data for Rocket classifier.\n",
    "    \"\"\"\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # Iterate through each CSV file\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "\n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Extract label and time-series data\n",
    "            label = df.iloc[0]['class']  # 'class' 열에서 라벨을 가져옵니다.\n",
    "            # label = df['class'].iloc[0]  # Assuming 'class' column exists in all files\n",
    "            time_series = df.iloc[:, 3:].values  # Exclude non-time-series columns\n",
    "            # time_series = time_series[:,:18]\n",
    "            time_series = pad_with_last_row(time_series,fixed_rows=10)\n",
    "            \n",
    "            x_data.append(time_series)\n",
    "            y_data.append(label)\n",
    "\n",
    "    return np.array(x_data), np.array(y_data)\n",
    "\n",
    "def infer_new_data(model_path, csv_file):\n",
    "    \"\"\"\n",
    "    Load a saved RocketTransformerClassifier model and perform inference on a new CSV file.\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "\n",
    "    # Load the saved model\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        rocket_classifier = pickle.load(f)\n",
    "\n",
    "    # Load and preprocess the new data\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract time-series data\n",
    "    time_series = df.iloc[:, 1:].values  # Exclude non-time-series columns\n",
    "    time_series = pad_with_last_row(time_series, fixed_rows=11)  # Ensure fixed number of rows\n",
    "\n",
    "    # Reshape for inference (Rocket expects 3D array: [samples, time_steps, features])\n",
    "    x_new = np.expand_dims(time_series, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Perform inference\n",
    "    transformer = rocket_classifier.classifiers_mapping[\"transformer\"]\n",
    "    scaler = rocket_classifier.classifiers_mapping[\"scaler\"]\n",
    "    classifier = rocket_classifier.classifiers_mapping[\"classifier\"]\n",
    "\n",
    "    # Transform and normalize the input data\n",
    "    x_new_transformed = transformer.transform(x_new)\n",
    "    x_new_transformed = scaler.transform(x_new_transformed)\n",
    "\n",
    "    # Predict class\n",
    "    prediction = classifier.predict(x_new_transformed)\n",
    "\n",
    "    return prediction  # Return the predicted label\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "def infer_top3_classes(model_path, csv_file):\n",
    "    \"\"\"\n",
    "    Load a saved RocketTransformerClassifier model and return the top-3 predicted classes with probabilities.\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "\n",
    "    # Load the saved model\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        rocket_classifier = pickle.load(f)\n",
    "\n",
    "    # Load and preprocess the new data\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract time-series data\n",
    "    time_series = df.iloc[:, 1:].values  # Exclude non-time-series columns\n",
    "    time_series = pad_with_last_row(time_series, fixed_rows=11)  # Ensure fixed number of rows\n",
    "\n",
    "    # Reshape for inference (Rocket expects 3D array: [samples, time_steps, features])\n",
    "    x_new = np.expand_dims(time_series, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Perform inference\n",
    "    transformer = rocket_classifier.classifiers_mapping[\"transformer\"]\n",
    "    scaler = rocket_classifier.classifiers_mapping[\"scaler\"]\n",
    "    classifier = rocket_classifier.classifiers_mapping[\"classifier\"]\n",
    "\n",
    "    # Transform and normalize the input data\n",
    "    x_new_transformed = transformer.transform(x_new)\n",
    "    x_new_transformed = scaler.transform(x_new_transformed)\n",
    "\n",
    "    # Get decision scores\n",
    "    decision_scores = classifier.decision_function(x_new_transformed)  # Shape: (1, num_classes)\n",
    "\n",
    "    # Convert decision scores to probabilities using softmax\n",
    "    probabilities = softmax(decision_scores, axis=1)[0]  # Shape: (num_classes,)\n",
    "\n",
    "    # Get top-3 classes and their probabilities\n",
    "    top3_indices = probabilities.argsort()[-3:][::-1]  # Indices of the top-3 probabilities\n",
    "    top3_classes = classifier.classes_[top3_indices]   # Corresponding class labels\n",
    "    top3_probabilities = probabilities[top3_indices]   # Corresponding probabilities\n",
    "\n",
    "    # Return as a dictionary\n",
    "    return [{\"class\": c, \"probability\": p} for c, p in zip(top3_classes, top3_probabilities)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이 셀 기반으로 위 셀 코드 만듦\n",
    "\n",
    "# Define Rocket Transformer Classifier\n",
    "class RocketTransformerClassifier:\n",
    "    def __init__(self):\n",
    "        self.classifiers_mapping = {}\n",
    "\n",
    "    def fit_rocket(self, x_train, y_train, kernels=20000):\n",
    "        # Initialize and fit Rocket transformer\n",
    "        rocket = Rocket(num_kernels=kernels, normalise=False)\n",
    "        rocket.fit(x_train)\n",
    "        x_training_transform = rocket.transform(x_train)\n",
    "\n",
    "        # Normalize the transformed data\n",
    "        scaler = StandardScaler()\n",
    "        x_training_transform = scaler.fit_transform(x_training_transform)\n",
    "\n",
    "        # Train RidgeClassifier with normalized transformed data\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n",
    "        classifier.fit(x_training_transform, y_train)\n",
    "\n",
    "        # Store the transformer, scaler, and classifier\n",
    "        self.classifiers_mapping[\"transformer\"] = rocket\n",
    "        self.classifiers_mapping[\"scaler\"] = scaler\n",
    "        self.classifiers_mapping[\"classifier\"] = classifier\n",
    "\n",
    "    def evaluate(self, x_val, y_val):\n",
    "        rocket = self.classifiers_mapping[\"transformer\"]\n",
    "        scaler = self.classifiers_mapping[\"scaler\"]\n",
    "        classifier = self.classifiers_mapping[\"classifier\"]\n",
    "    \n",
    "        # Transform and normalize test data\n",
    "        x_val_transform = rocket.transform(x_val)\n",
    "        x_val_transform = scaler.transform(x_val_transform)\n",
    "    \n",
    "        # Predict and evaluate\n",
    "        predictions = classifier.predict(x_val_transform)\n",
    "        accuracy = metrics.accuracy_score(y_val, predictions)\n",
    "\n",
    "        logger.info(\"-----------------------------------------------\")\n",
    "        logger.info(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def predict_rocket(self, x_test, y_test):\n",
    "        # Retrieve transformer, scaler, and classifier\n",
    "        rocket = self.classifiers_mapping[\"transformer\"]\n",
    "        scaler = self.classifiers_mapping[\"scaler\"]\n",
    "        classifier = self.classifiers_mapping[\"classifier\"]\n",
    "    \n",
    "        # Transform and normalize test data\n",
    "        x_test_transform = rocket.transform(x_test)\n",
    "        x_test_transform = scaler.transform(x_test_transform)\n",
    "    \n",
    "        # Predict and evaluate\n",
    "        predictions = classifier.predict(x_test_transform)\n",
    "        accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "        confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "        classification_report = metrics.classification_report(y_test, predictions)\n",
    "    \n",
    "        logger.info(\"-----------------------------------------------\")\n",
    "        logger.info(f\"Accuracy: {accuracy}\")\n",
    "        logger.info(\"\\nConfusion Matrix:\\n\" + str(confusion_matrix))\n",
    "        logger.info(\"\\nClassification Report:\\n\" + classification_report)\n",
    "    \n",
    "        return accuracy, confusion_matrix, classification_report\n",
    "    \n",
    "def pad_with_last_row(series, fixed_rows):\n",
    "    \"\"\"\n",
    "    Pad time-series data with the last row to ensure a fixed number of rows.\n",
    "    Truncate rows if too many, pad with the last row if too few.\n",
    "    \"\"\"\n",
    "    series = np.array(series)\n",
    "    current_rows, columns = series.shape\n",
    "\n",
    "    # Truncate if too many rows\n",
    "    if current_rows > fixed_rows:\n",
    "        return series[:fixed_rows, :]\n",
    "\n",
    "    # Pad with the last row if too few rows\n",
    "    if current_rows < fixed_rows:\n",
    "        last_row = series[-1, :]  # Get the last row\n",
    "        padding = np.tile(last_row, (fixed_rows - current_rows, 1))  # Repeat the last row\n",
    "        return np.vstack((series, padding))  # Add padding rows\n",
    "\n",
    "    return series\n",
    "\n",
    "def load_time_series_data(input_dir, max_len = 11):\n",
    "    \"\"\"\n",
    "    Load time-series CSV files from 377 to 382 and ensure all have the same rows and columns.\n",
    "\n",
    "    Parameters:\n",
    "        input_dir (str): Directory containing CSV files.\n",
    "        target_views (list): List of target views (e.g., ['A', 'B', 'C']).\n",
    "        fixed_rows (int): Fixed number of rows for each time-series.\n",
    "\n",
    "    Returns:\n",
    "        x_data (np.array): Filtered and formatted time-series data containing only 'y' values.\n",
    "        y_data (np.array): Corresponding labels.\n",
    "    \"\"\"\n",
    "    # Include only files with numbers 377 to 382\n",
    "    included_range = range(377, 383)\n",
    "\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Extract the number (377, 378, etc.) from the file name\n",
    "            file_number = int(file_name.split('_')[0])\n",
    "\n",
    "            # Skip files not in the included range\n",
    "            if file_number not in included_range:\n",
    "                continue\n",
    "\n",
    "            # Extract class label, view, and person_id from file name\n",
    "            class_label = file_name.split('_')[0]  # First part of the file name\n",
    "\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "            # 총 33개의 그룹에서 y 인덱스만 선택\n",
    "            time_series = df.iloc[:, 3:].values  # Exclude metadata columns\n",
    "            # time_series = time_series[:,:18]\n",
    "\n",
    "            # Fix the number of rows\n",
    "            time_series = pad_with_last_row(time_series,max_len)\n",
    "\n",
    "            x_data.append(time_series)\n",
    "            y_data.append(class_label)\n",
    "\n",
    "    return np.array(x_data), np.array(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data shape: (383, 11, 36)\n",
      "y_data shape: (383,)\n",
      "Sample label: 377\n",
      "Sample time-series data shape: (11, 36)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "input_dir = '/root/juno/peak_detection_results/segments2'  # 데이터 디렉토리 경로\n",
    "x_data, y_data = load_time_series_data(input_dir, max_len = 11)\n",
    "\n",
    "# 데이터 확인\n",
    "print(f\"x_data shape: {x_data.shape}\")\n",
    "print(f\"y_data shape: {y_data.shape}\")\n",
    "print(f\"Sample label: {y_data[0]}\")\n",
    "print(f\"Sample time-series data shape: {x_data[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_temp, y_train, y_temp = train_test_split(x_data, y_data, test_size=0.2, random_state=42, stratify=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Rocket classifier...\n"
     ]
    }
   ],
   "source": [
    "# Initialize Rocket classifier\n",
    "logger.info(\"Initializing Rocket classifier...\")\n",
    "rocket_classifier = RocketTransformerClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:-----------------------------------------------\n",
      "INFO:__main__:Accuracy: 0.7368421052631579\n",
      "Training Progress:  10%|█         | 1/10 [00:10<01:38, 10.95s/it, Val Accuracy=0.7368]INFO:__main__:New best model saved with val_accuracy: 0.7368\n",
      "INFO:__main__:-----------------------------------------------\n",
      "INFO:__main__:Accuracy: 0.7894736842105263\n",
      "Training Progress:  20%|██        | 2/10 [00:21<01:27, 10.90s/it, Val Accuracy=0.7895]INFO:__main__:New best model saved with val_accuracy: 0.7895\n",
      "INFO:__main__:-----------------------------------------------\n",
      "INFO:__main__:Accuracy: 0.7631578947368421\n",
      "Training Progress:  30%|███       | 3/10 [00:32<01:16, 10.94s/it, Val Accuracy=0.7632]INFO:__main__:-----------------------------------------------\n",
      "INFO:__main__:Accuracy: 0.7894736842105263\n",
      "Training Progress:  40%|████      | 4/10 [00:43<01:05, 10.92s/it, Val Accuracy=0.7895]INFO:__main__:-----------------------------------------------\n",
      "INFO:__main__:Accuracy: 0.7631578947368421\n",
      "Training Progress:  50%|█████     | 5/10 [00:54<00:54, 10.92s/it, Val Accuracy=0.7632]INFO:__main__:-----------------------------------------------\n",
      "INFO:__main__:Accuracy: 0.7894736842105263\n",
      "Training Progress:  60%|██████    | 6/10 [01:05<00:43, 10.97s/it, Val Accuracy=0.7895]INFO:__main__:-----------------------------------------------\n",
      "INFO:__main__:Accuracy: 0.7631578947368421\n",
      "Training Progress:  70%|███████   | 7/10 [01:16<00:32, 10.98s/it, Val Accuracy=0.7632]INFO:__main__:Early stopping triggered.\n",
      "Training Progress:  70%|███████   | 7/10 [01:16<00:32, 10.96s/it, Val Accuracy=0.7632]\n",
      "INFO:__main__:Best model loaded with val_accuracy: 0.7895\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "best_val_accuracy = 0\n",
    "patience = 5\n",
    "epochs_without_improvement = 0\n",
    "max_epochs = 10\n",
    "\n",
    "# 저장할 모델 파일 경로\n",
    "best_model_path = \"lateralraise_fin.pkl\"\n",
    "\n",
    "# tqdm을 사용하여 학습률 표시\n",
    "with tqdm(total=max_epochs, desc=\"Training Progress\") as pbar:\n",
    "    for epoch in range(max_epochs):\n",
    "        # Train the model\n",
    "        rocket_classifier.fit_rocket(x_train, y_train)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_accuracy = rocket_classifier.evaluate(x_val, y_val)\n",
    "\n",
    "        # Update tqdm bar with validation accuracy\n",
    "        pbar.set_postfix({\"Val Accuracy\": f\"{val_accuracy:.4f}\"})\n",
    "        pbar.update(1)  # Progress bar 업데이트\n",
    "\n",
    "        # Save the best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            epochs_without_improvement = 0\n",
    "\n",
    "            # Save the model\n",
    "            with open(best_model_path, \"wb\") as f:\n",
    "                pickle.dump(rocket_classifier, f)\n",
    "            logger.info(f\"New best model saved with val_accuracy: {best_val_accuracy:.4f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Early stopping logic\n",
    "        if epochs_without_improvement >= patience:\n",
    "            logger.info(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model after training\n",
    "with open(best_model_path, \"rb\") as f:\n",
    "    best_model = pickle.load(f)\n",
    "\n",
    "logger.info(f\"Best model loaded with val_accuracy: {best_val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading the best saved model for testing...\n",
      "INFO:__main__:Testing the best Rocket classifier...\n",
      "INFO:__main__:-----------------------------------------------\n",
      "INFO:__main__:Accuracy: 0.7692307692307693\n",
      "INFO:__main__:\n",
      "Confusion Matrix:\n",
      "[[6 0 0 0 0 0]\n",
      " [0 5 1 0 0 2]\n",
      " [0 0 4 0 1 2]\n",
      " [0 0 0 5 0 0]\n",
      " [0 1 0 0 5 0]\n",
      " [1 1 0 0 0 5]]\n",
      "INFO:__main__:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         377       0.86      1.00      0.92         6\n",
      "         378       0.71      0.62      0.67         8\n",
      "         379       0.80      0.57      0.67         7\n",
      "         380       1.00      1.00      1.00         5\n",
      "         381       0.83      0.83      0.83         6\n",
      "         382       0.56      0.71      0.62         7\n",
      "\n",
      "    accuracy                           0.77        39\n",
      "   macro avg       0.79      0.79      0.79        39\n",
      "weighted avg       0.78      0.77      0.77        39\n",
      "\n",
      "INFO:__main__:Test Accuracy: 0.7692\n",
      "INFO:__main__:\n",
      "Confusion Matrix:\n",
      "[[6 0 0 0 0 0]\n",
      " [0 5 1 0 0 2]\n",
      " [0 0 4 0 1 2]\n",
      " [0 0 0 5 0 0]\n",
      " [0 1 0 0 5 0]\n",
      " [1 1 0 0 0 5]]\n",
      "INFO:__main__:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         377       0.86      1.00      0.92         6\n",
      "         378       0.71      0.62      0.67         8\n",
      "         379       0.80      0.57      0.67         7\n",
      "         380       1.00      1.00      1.00         5\n",
      "         381       0.83      0.83      0.83         6\n",
      "         382       0.56      0.71      0.62         7\n",
      "\n",
      "    accuracy                           0.77        39\n",
      "   macro avg       0.79      0.79      0.79        39\n",
      "weighted avg       0.78      0.77      0.77        39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 저장된 최고 성능 모델 파일 경로\n",
    "best_model_path = \"lateralraise_fin.pkl\"\n",
    "\n",
    "# 저장된 모델 로드\n",
    "logger.info(\"Loading the best saved model for testing...\")\n",
    "with open(best_model_path, \"rb\") as f:\n",
    "    best_rocket_classifier = pickle.load(f)\n",
    "\n",
    "# 테스트 수행\n",
    "logger.info(\"Testing the best Rocket classifier...\")\n",
    "accuracy, confusion_matrix, classification_report = best_rocket_classifier.predict_rocket(x_test, y_test)\n",
    "\n",
    "# 테스트 결과 출력\n",
    "logger.info(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "logger.info(\"\\nConfusion Matrix:\\n\" + str(confusion_matrix))\n",
    "logger.info(\"\\nClassification Report:\\n\" + classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for the new data: 377\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "model_path = \"/root/juno/lateralraise_fin.pkl\"  # 저장된 모델 경로\n",
    "new_csv_file = \"/root/test2.csv\"  # 새 데이터 CSV 파일 경로\n",
    "\n",
    "# Perform inference\n",
    "predicted_label = infer_new_data(model_path, new_csv_file)[0]\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted label for the new data: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 테스트용, 별 의미 없음.\n",
    "\n",
    "# Define paths\n",
    "model_path = \"/root/juno/lateralraise.pkl\"  # 저장된 모델 경로\n",
    "result = []\n",
    "\n",
    "for i in range(2):\n",
    "    new_csv_file = f\"/root/wrong_{i}.csv\"  # 새 데이터 CSV 파일 경로\n",
    "\n",
    "    # Perform inference\n",
    "    predicted_label = infer_new_data(model_path, new_csv_file)[0]\n",
    "    result.append(predicted_label)\n",
    "\n",
    "    # Print the result\n",
    "    print(f\"Predicted label for the new data: {predicted_label}\")\n",
    "\n",
    "dict = {'377': '정자세','378':'무릎 반동'}\n",
    "\n",
    "new = [dict[i] for i in result]\n",
    "\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Class = 377, Probability = 0.5192\n",
      "Rank 2: Class = 378, Probability = 0.3442\n",
      "Rank 3: Class = 380, Probability = 0.0673\n"
     ]
    }
   ],
   "source": [
    "### 테스트용, 별 의미 없음.\n",
    "\n",
    "model_path = \"/root/juno/lateralraise.pkl\"  # 저장된 모델 경로\n",
    "new_csv_file = \"/root/test2.csv\"  # 새 데이터 CSV 파일 경로\n",
    "\n",
    "# Perform inference\n",
    "top3_predictions = infer_top3_classes(model_path, new_csv_file)\n",
    "\n",
    "# Print the results\n",
    "for rank, result in enumerate(top3_predictions, start=1):\n",
    "    print(f\"Rank {rank}: Class = {result['class']}, Probability = {result['probability']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위가 수현이가 고쳐준 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래는 수현이가 고치기 전 원래 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Results saved to: /root/juno/classification_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a file\n",
    "output_dir = '/root/juno'\n",
    "\n",
    "results_file = os.path.join(output_dir, 'classification_results.txt')\n",
    "with open(results_file, 'w') as f:\n",
    "    f.write(\"Accuracy: {}\\n\".format(accuracy))\n",
    "    f.write(\"\\nConfusion Matrix:\\n\")\n",
    "    f.write(str(confusion_matrix))\n",
    "    f.write(\"\\nClassification Report:\\n\")\n",
    "    f.write(classification_report)\n",
    "\n",
    "logger.info(\"Results saved to: \" + results_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 함수\n",
    "def save_model(model, output_path):\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    logger.info(f\"Model saved to {output_path}\")\n",
    "\n",
    "# 모델 로드 함수\n",
    "def load_model(input_path):\n",
    "    with open(input_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    logger.info(f\"Model loaded from {input_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 파일 경로\n",
    "model_save_path = os.path.join(output_dir, 'rocket_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saving the trained model...\n",
      "INFO:__main__:Model saved to /root/juno/rocket_classifier.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train 완료 후 모델 저장\n",
    "logger.info(\"Saving the trained model...\")\n",
    "save_model(rocket_classifier, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading the model for testing...\n",
      "INFO:__main__:Model loaded from /root/juno/rocket_classifier.pkl\n"
     ]
    }
   ],
   "source": [
    "# 이후 필요할 때 모델을 로드하여 사용 가능\n",
    "logger.info(\"Loading the model for testing...\")\n",
    "loaded_classifier = load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/condatest/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/envs/condatest/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/root/miniconda3/envs/condatest/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "INFO:__main__:-----------------------------------------------\n",
      "INFO:__main__:Accuracy: 0.7\n",
      "INFO:__main__:\n",
      "Confusion Matrix:\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "INFO:__main__:\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         377       0.50      0.50      0.50         2\n",
      "         378       0.50      1.00      0.67         2\n",
      "         379       1.00      0.50      0.67         2\n",
      "         380       0.67      1.00      0.80         2\n",
      "         381       1.00      1.00      1.00         2\n",
      "         382       0.00      0.00      0.00         2\n",
      "         383       0.75      1.00      0.86         3\n",
      "         384       0.50      0.33      0.40         3\n",
      "         385       1.00      0.50      0.67         2\n",
      "         386       0.50      0.50      0.50         2\n",
      "         387       1.00      0.67      0.80         3\n",
      "         388       0.67      1.00      0.80         2\n",
      "         389       0.67      1.00      0.80         2\n",
      "         390       0.50      0.50      0.50         2\n",
      "         391       0.00      0.00      0.00         2\n",
      "         392       1.00      1.00      1.00         2\n",
      "         393       0.67      1.00      0.80         2\n",
      "         394       0.50      1.00      0.67         2\n",
      "         395       0.50      0.50      0.50         2\n",
      "         396       1.00      1.00      1.00         2\n",
      "         397       1.00      0.33      0.50         3\n",
      "         398       1.00      1.00      1.00         2\n",
      "         399       0.67      0.67      0.67         3\n",
      "         400       0.67      1.00      0.80         2\n",
      "         401       1.00      0.67      0.80         3\n",
      "         402       0.00      0.00      0.00         2\n",
      "         403       1.00      1.00      1.00         2\n",
      "         404       1.00      1.00      1.00         2\n",
      "         405       1.00      1.00      1.00         2\n",
      "         406       0.67      1.00      0.80         2\n",
      "         407       0.33      0.50      0.40         2\n",
      "         408       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.70        70\n",
      "   macro avg       0.70      0.71      0.67        70\n",
      "weighted avg       0.71      0.70      0.67        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 로드된 모델로 테스트 수행\n",
    "accuracy, confusion_matrix, classification_report = loaded_classifier.predict_rocket(x_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
